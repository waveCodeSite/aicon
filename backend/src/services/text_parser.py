"""
文本解析服务 - 智能章节识别和内容解析
严格按照data-model.md规范实现，支持多种章节标记格式
"""

import re
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
import logging

try:
    from src.core.logging import get_logger
    from src.models.chapter import Chapter, ChapterStatus
    from src.models.paragraph import Paragraph, ParagraphAction
    from src.models.sentence import Sentence, SentenceStatus
except ImportError:
    # 用于独立测试的情况
    def get_logger(name):
        import logging
        return logging.getLogger(name)

    # 创建简单的枚举模拟
    class ChapterStatus:
        PENDING = "pending"

    class ParagraphAction:
        KEEP = "keep"

    class SentenceStatus:
        PENDING = "pending"

logger = get_logger(__name__)


@dataclass
class ChapterDetection:
    """章节检测结果"""
    title: str
    content: str
    chapter_number: int
    start_position: int
    end_position: int
    detection_method: str  # regex, ml, hybrid
    confidence_score: float = 0.0


@dataclass
class ParsedContent:
    """解析结果结构"""
    chapters: List[ChapterDetection]
    paragraphs: List[str]
    sentences: List[str]
    metadata: Dict[str, Any]
    processing_time: float = 0.0


class ChapterDetector(ABC):
    """章节检测器抽象基类"""

    @abstractmethod
    def detect_chapters(self, text: str) -> List[ChapterDetection]:
        """检测章节"""
        pass


class RegexChapterDetector(ChapterDetector):
    """基于正则表达式的章节检测器"""

    def __init__(self):
        # 多种章节标记模式，按优先级排序
        self.patterns = [
            # 阿拉伯数字章节：第1章、第一章、Chapter 1
            {
                'pattern': r'^第[一二三四五六七八九十百千万0-9]+[章节回卷篇]',
                'name': 'chinese_numbered',
                'confidence': 0.9
            },
            # 数字章节：1. 第一章、1、Chapter 1
            {
                'pattern': r'^(\d+)\.?\s*(第?[一二三四五六七八九十百千万0-9]*[章节回卷篇]|Chapter\s*\d+|[一二三四五六七八九十百千万]+、)',
                'name': 'numbered',
                'confidence': 0.85
            },
            # 英文章节：Chapter 1, Part 1
            {
                'pattern': r'^(Chapter|Part|Section)\s+\d+',
                'name': 'english',
                'confidence': 0.8
            },
            # 简单数字标记：1、2、3、
            {
                'pattern': r'^(\d+)、',
                'name': 'simple_numbered',
                'confidence': 0.7
            },
            # 括号章节：（一）、[第一卷]
            {
                'pattern': r'^[【\(]\s*[第]?[一二三四五六七八九十百千万0-9]+\s*[章节回卷篇]\s*[】\)]',
                'name': 'bracketed',
                'confidence': 0.75
            }
        ]

        # 编译正则表达式
        self.compiled_patterns = []
        for pattern_info in self.patterns:
            try:
                compiled = re.compile(
                    pattern_info['pattern'],
                    re.MULTILINE | re.IGNORECASE
                )
                self.compiled_patterns.append({
                    'compiled': compiled,
                    'name': pattern_info['name'],
                    'confidence': pattern_info['confidence']
                })
            except re.error as e:
                logger.warning(f"正则表达式编译失败: {pattern_info['pattern']}, 错误: {e}")

    def detect_chapters(self, text: str) -> List[ChapterDetection]:
        """使用正则表达式检测章节"""
        chapters = []
        lines = text.split('\n')

        chapter_number = 1
        current_position = 0
        chapter_start_positions = []

        # 查找所有可能的章节标题
        for i, line in enumerate(lines):
            line = line.strip()
            if not line:
                current_position += len(lines[i]) + 1
                continue

            # 尝试匹配所有模式
            for pattern in self.compiled_patterns:
                match = pattern['compiled'].match(line)
                if match:
                    chapter_start_positions.append({
                        'line_index': i,
                        'title': line,
                        'position': current_position,
                        'confidence': pattern['confidence'],
                        'method': pattern['name']
                    })
                    break

            current_position += len(lines[i]) + 1

        # 如果没有检测到章节，创建单个章节
        if not chapter_start_positions:
            return [ChapterDetection(
                title="完整文档",
                content=text,
                chapter_number=1,
                start_position=0,
                end_position=len(text),
                detection_method="fallback",
                confidence_score=0.5
            )]

        # 构建章节内容
        for i, chapter_info in enumerate(chapter_start_positions):
            start_pos = chapter_info['position']

            # 确定章节结束位置
            if i < len(chapter_start_positions) - 1:
                end_pos = chapter_start_positions[i + 1]['position']
            else:
                end_pos = len(text)

            # 提取章节内容
            chapter_content = text[start_pos:end_pos].strip()

            # 移除标题行，只保留内容
            title_end = chapter_content.find('\n')
            if title_end > 0:
                content_only = chapter_content[title_end + 1:].strip()
            else:
                content_only = ""

            chapters.append(ChapterDetection(
                title=chapter_info['title'],
                content=content_only,
                chapter_number=chapter_number,
                start_position=start_pos,
                end_position=end_pos,
                detection_method=chapter_info['method'],
                confidence_score=chapter_info['confidence']
            ))

            chapter_number += 1

        return chapters


class TextSplitter:
    """文本分割工具类"""

    @staticmethod
    def split_into_paragraphs(text: str) -> List[str]:
        """将文本分割为段落"""
        if not text:
            return []

        # 标准化换行符
        text = text.replace('\r\n', '\n').replace('\r', '\n')

        # 分割为段落
        paragraphs = []
        lines = text.split('\n')
        current_paragraph = []

        for line in lines:
            line = line.strip()
            if line:
                current_paragraph.append(line)
            elif current_paragraph:
                # 遇到空行，结束当前段落
                paragraph = ' '.join(current_paragraph)
                if paragraph:
                    paragraphs.append(paragraph)
                current_paragraph = []

        # 添加最后一个段落
        if current_paragraph:
            paragraph = ' '.join(current_paragraph)
            if paragraph:
                paragraphs.append(paragraph)

        return paragraphs

    @staticmethod
    def split_into_sentences(text: str) -> List[str]:
        """将文本分割为句子"""
        if not text:
            return []

        # 句子结束标记
        sentence_endings = re.compile(r'[。！？.!?]+')

        sentences = []
        start = 0

        for match in sentence_endings.finditer(text):
            end = match.end()
            sentence = text[start:end].strip()

            if sentence and len(sentence) > 2:  # 过滤过短的句子
                sentences.append(sentence)

            start = end

        # 处理最后部分（如果没有句号结尾）
        if start < len(text):
            remaining = text[start:].strip()
            if remaining and len(remaining) > 2:
                sentences.append(remaining)

        return sentences


class TextParserService:
    """文本解析服务主类"""

    def __init__(self):
        self.detector = RegexChapterDetector()
        self.splitter = TextSplitter()

        # 统计信息
        self.stats = {
            'total_documents_processed': 0,
            'total_chapters_detected': 0,
            'average_chapters_per_document': 0.0,
            'detection_accuracy': 0.0
        }

    async def parse_document(self, text: str, options: Optional[Dict[str, Any]] = None) -> ParsedContent:
        """
        解析文档，识别章节、段落和句子

        Args:
            text: 待解析的文本内容
            options: 解析选项
                - min_chapter_length: 最小章节长度（默认1000字符）
                - preserve_structure: 保持原始结构（默认True）
                - language: 语言设置（默认'zh'）

        Returns:
            ParsedContent: 解析结果
        """
        import time
        start_time = time.time()

        if not text or not text.strip():
            raise ValueError("文本内容不能为空")

        options = options or {}
        min_chapter_length = options.get('min_chapter_length', 1000)

        try:
            logger.info(f"开始解析文档，文本长度: {len(text)} 字符")

            # 1. 检测章节
            chapters = self.detector.detect_chapters(text)

            # 2. 如果章节太长，尝试进一步分割
            if len(chapters) == 1 and len(text) > min_chapter_length * 2:
                logger.info("单个章节过长，尝试智能分割")
                chapters = self._split_long_chapter(text)

            # 3. 为每个章节分割段落和句子
            all_paragraphs = []
            all_sentences = []

            for chapter in chapters:
                # 分割段落
                paragraphs = self.splitter.split_into_paragraphs(chapter.content)
                all_paragraphs.extend(paragraphs)

                # 分割句子
                for paragraph in paragraphs:
                    sentences = self.splitter.split_into_sentences(paragraph)
                    all_sentences.extend(sentences)

            # 4. 构建元数据
            metadata = {
                'total_length': len(text),
                'chapter_count': len(chapters),
                'paragraph_count': len(all_paragraphs),
                'sentence_count': len(all_sentences),
                'detection_methods': list(set(c.detection_method for c in chapters)),
                'average_confidence': sum(c.confidence_score for c in chapters) / len(chapters) if chapters else 0,
                'options_used': options
            }

            processing_time = time.time() - start_time

            # 5. 更新统计信息
            self._update_stats(len(chapters))

            result = ParsedContent(
                chapters=chapters,
                paragraphs=all_paragraphs,
                sentences=all_sentences,
                metadata=metadata,
                processing_time=processing_time
            )

            logger.info(f"文档解析完成: {len(chapters)} 章节, {len(all_paragraphs)} 段落, {len(all_sentences)} 句子, 耗时: {processing_time:.2f}s")

            return result

        except Exception as e:
            logger.error(f"文档解析失败: {str(e)}")
            raise

    def _split_long_chapter(self, text: str) -> List[ChapterDetection]:
        """分割过长的章节"""
        # 尝试按照语义边界分割
        split_patterns = [
            r'\n\s*\n',  # 双换行
            r'[。！？]\s*\n',  # 句号+换行
            r'[。！？]\s{2,}',  # 句号+多个空格
        ]

        chapters = []
        best_split_positions = []

        # 寻找最佳分割点
        for pattern in split_patterns:
            matches = list(re.finditer(pattern, text))
            if len(matches) >= 2:  # 至少找到2个分割点
                positions = [m.start() for m in matches]
                # 均匀选择分割点
                target_chapters = max(len(text) // 10000, 2)  # 每10k字符一个章节
                step = max(len(positions) // (target_chapters - 1), 1)
                selected_positions = [positions[i] for i in range(0, len(positions), step)]
                best_split_positions = selected_positions
                break

        if not best_split_positions:
            # 如果没有找到好的分割点，按固定长度分割
            chunk_size = 10000
            best_split_positions = list(range(chunk_size, len(text), chunk_size))

        # 创建章节
        prev_pos = 0
        chapter_num = 1

        for pos in best_split_positions:
            if pos > prev_pos + 1000:  # 确保章节有足够长度
                content = text[prev_pos:pos].strip()
                if content:
                    chapters.append(ChapterDetection(
                        title=f"第{chapter_num}部分",
                        content=content,
                        chapter_number=chapter_num,
                        start_position=prev_pos,
                        end_position=pos,
                        detection_method="auto_split",
                        confidence_score=0.6
                    ))
                    chapter_num += 1
                prev_pos = pos

        # 添加最后一部分
        if prev_pos < len(text):
            content = text[prev_pos:].strip()
            if content:
                chapters.append(ChapterDetection(
                    title=f"第{chapter_num}部分",
                    content=content,
                    chapter_number=chapter_num,
                    start_position=prev_pos,
                    end_position=len(text),
                    detection_method="auto_split",
                    confidence_score=0.6
                ))

        return chapters

    def _update_stats(self, chapter_count: int):
        """更新统计信息"""
        self.stats['total_documents_processed'] += 1
        self.stats['total_chapters_detected'] += chapter_count
        self.stats['average_chapters_per_document'] = (
            self.stats['total_chapters_detected'] /
            self.stats['total_documents_processed']
        )

    async def parse_to_models(self, project_id: str, text: str, options: Optional[Dict[str, Any]] = None) -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """
        解析文本并转换为数据库模型格式

        Args:
            project_id: 项目ID
            text: 待解析文本
            options: 解析选项

        Returns:
            (chapters_data, paragraphs_data, sentences_data): 三层数据结构
        """
        parsed = await self.parse_document(text, options)

        chapters_data = []
        paragraphs_data = []
        sentences_data = []

        # 构建章节数据
        for chapter_detection in parsed.chapters:
            chapter_data = {
                'project_id': project_id,
                'title': chapter_detection.title,
                'content': chapter_detection.content,
                'chapter_number': chapter_detection.chapter_number,
                'word_count': len(chapter_detection.content),
                'paragraph_count': 0,  # 稍后更新
                'sentence_count': 0,   # 稍后更新
                'status': ChapterStatus.PENDING.value,
                'is_confirmed': False,
                'generation_settings': None,
                'created_at': None,  # 由数据库自动设置
                'updated_at': None   # 由数据库自动设置
            }
            chapters_data.append(chapter_data)

        # 构建段落和句子的映射关系
        chapter_index = 0
        paragraph_index = 0
        sentence_index = 0

        for chapter_idx, chapter_detection in enumerate(parsed.chapters):
            # 获取当前章节的段落
            chapter_paragraphs = self.splitter.split_into_paragraphs(chapter_detection.content)

            chapter_sentence_count = 0

            for para_idx, paragraph_text in enumerate(chapter_paragraphs):
                paragraph_data = {
                    'chapter_id': None,  # 在保存后设置
                    'content': paragraph_text,
                    'order_index': para_idx + 1,
                    'word_count': len(paragraph_text),
                    'sentence_count': 0,  # 稍后更新
                    'action': ParagraphAction.KEEP.value,
                    'edited_content': None,
                    'is_confirmed': False,
                    'ignore_reason': None,
                    'audio_url': None,
                    'audio_duration': None,
                    'created_at': None,
                    'updated_at': None
                }
                paragraphs_data.append(paragraph_data)

                # 获取当前段落的句子
                paragraph_sentences = self.splitter.split_into_sentences(paragraph_text)
                chapter_sentence_count += len(paragraph_sentences)
                paragraph_data['sentence_count'] = len(paragraph_sentences)

                for sent_idx, sentence_text in enumerate(paragraph_sentences):
                    sentence_data = {
                        'paragraph_id': None,  # 在保存后设置
                        'content': sentence_text,
                        'order_index': sent_idx + 1,
                        'word_count': len(sentence_text),
                        'character_count': len(sentence_text),
                        'image_url': None,
                        'image_prompt': None,
                        'audio_url': None,
                        'start_time': None,
                        'end_time': None,
                        'duration': None,
                        'confidence_score': None,
                        'voice_settings': None,
                        'voice_type': None,
                        'speech_rate': 1.0,
                        'pitch': 1.0,
                        'volume': 1.0,
                        'status': SentenceStatus.PENDING.value,
                        'error_message': None,
                        'retry_count': 0,
                        'edited_content': None,
                        'edited_prompt': None,
                        'is_manual_edited': False,
                        'created_at': None,
                        'updated_at': None,
                        'completed_at': None
                    }
                    sentences_data.append(sentence_data)
                    sentence_index += 1

                paragraph_index += 1

            # 更新章节数据
            chapters_data[chapter_idx]['paragraph_count'] = len(chapter_paragraphs)
            chapters_data[chapter_idx]['sentence_count'] = chapter_sentence_count
            chapter_index += 1

        logger.info(f"解析完成: {len(chapters_data)} 章节, {len(paragraphs_data)} 段落, {len(sentences_data)} 句子")

        return chapters_data, paragraphs_data, sentences_data

    def get_detection_stats(self) -> Dict[str, Any]:
        """获取检测统计信息"""
        return self.stats.copy()

    async def validate_chapter_detection(self, text: str, expected_chapters: Optional[int] = None) -> Dict[str, Any]:
        """
        验证章节检测结果

        Args:
            text: 待检测文本
            expected_chapters: 预期章节数量（可选）

        Returns:
            验证结果
        """
        try:
            parsed = await self.parse_document(text)

            validation_result = {
                'is_valid': True,
                'detected_chapters': len(parsed.chapters),
                'expected_chapters': expected_chapters,
                'confidence_scores': [c.confidence_score for c in parsed.chapters],
                'detection_methods': list(set(c.detection_method for c in parsed.chapters)),
                'average_confidence': parsed.metadata.get('average_confidence', 0),
                'chapter_titles': [c.title for c in parsed.chapters],
                'warnings': [],
                'recommendations': []
            }

            # 检查预期章节数量
            if expected_chapters and expected_chapters != len(parsed.chapters):
                validation_result['warnings'].append(
                    f"检测章节数({len(parsed.chapters)})与预期({expected_chapters})不符"
                )

            # 检查置信度
            low_confidence_chapters = [c for c in parsed.chapters if c.confidence_score < 0.7]
            if low_confidence_chapters:
                validation_result['warnings'].append(
                    f"发现{len(low_confidence_chapters)}个低置信度章节"
                )
                validation_result['recommendations'].append(
                    "建议手动检查低置信度章节的准确性"
                )

            # 检查检测方法多样性
            if len(set(c.detection_method for c in parsed.chapters)) > 2:
                validation_result['recommendations'].append(
                    "检测方法多样，建议统一章节标记格式"
                )

            return validation_result

        except Exception as e:
            return {
                'is_valid': False,
                'error': str(e),
                'warnings': [],
                'recommendations': ["请检查文本格式和内容"]
            }


# 全局实例
text_parser_service = TextParserService()

__all__ = [
    'TextParserService',
    'ChapterDetection',
    'ParsedContent',
    'RegexChapterDetector',
    'TextSplitter',
    'text_parser_service'
]